{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk \n",
    "\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "#nltk.download('punkt')\n",
    "\n",
    "with open(r\"E:\\Semesters\\Spring_2022_break\\Machine Learning\\labels.json\") as f:\n",
    "  data = json.load(f)\n",
    "\n",
    "  \n",
    "#convert DICT to String\n",
    "data1 =json.dumps(data)\n",
    "\n",
    "parsed = json.loads(data1)\n",
    "print(parsed['good'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(r\"E:\\Semesters\\Spring_2022_break\\Machine Learning\\labels.json\") as f:\n",
    "  data = json.load(f)\n",
    "  lables =json.dumps(data)\n",
    "  lables_parsed = json.loads(lables)\n",
    "\n",
    "with open(r\"E:\\Semesters\\Spring_2022_break\\Machine Learning\\emoji.json\", encoding=\"utf8\") as f:\n",
    "  data_emojies = json.load(f)\n",
    "  #print(data_emojies)\n",
    "data1 = json.dumps(data_emojies)\n",
    "emoji_parsed = json.loads(data1)\n",
    "\n",
    "lables_parsed.update(emoji_parsed)\n",
    "\n",
    "msg = \"nice\"\n",
    "\n",
    "msgarray = msg.split()\n",
    "msgvalue=0\n",
    "for i in range(len(msgarray)):\n",
    "    try:\n",
    "    #  print(parsed[msgarray[i]])\n",
    "     msgvalue+=parsed[msgarray[i]]\n",
    "     \n",
    "    except:\n",
    "      pass\n",
    "      # print(msgarray[i], \"not found\")\n",
    "      # print()\n",
    "       #print(msgvalue)\n",
    "if (msgvalue == 0):\n",
    "    print('Neutral')\n",
    "    print(msgvalue)\n",
    "elif msgvalue < 1:\n",
    "    print('negative value')\n",
    "    print(msgvalue)\n",
    "else:\n",
    "    print('positive')\n",
    "    print(msgvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def text_has_emoji(test_input1):\n",
    "    for character in test_input1:\n",
    "        if ord(character) > 127:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "test_input1 = \"good üòÇ\"\n",
    "print(text_has_emoji(test_input1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,r\"C:\\Users\\Abdul\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\")\n",
    "import pandas  as pd\n",
    "import csv\n",
    "\n",
    "data_xls = pd.read_csv(r'E:\\Semesters\\Spring_2022_break\\Machine Learning\\TEST1.CSV')\n",
    "#data_xls.to_csv(r'C:\\Users\\Abdul\\Downloads\\AI DATASET\\Dataset11.csv',index=False)\n",
    "#df = pd.read_csv(r'C:\\Users\\Abdul\\Downloads\\AI DATASET\\Book3CSV.csv')\n",
    "#df['Comments']\n",
    "\n",
    "#data_xls\n",
    "#df = pd.read_csv(\"input.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Comments', 'Sarcasm_Y_N'], ['Sorry No Comments ..I came here to read comments ..üòú', '1'], [\"Hi guys. I'm so happy and proud of myself and I thought I should share this with you!!! Today, I saw myself on TV, when I turned it off. üòùüòù\", '1'], ['Hahahaha your intelligence üòúüòúüòú', '1'], ['Aqsa Naveed we were proud backbenchers üòúüòúüòÇ', '1'], ['Hemant extraordinary sketcherüòù..right Ujjawal??', '1'], ['Boys should respect them..!! üòúüî´', '1'], ['Your love broüòú', '1'], ['Bro u r handsome üòùü§£ü§£', '1'], [\"This talent I don't have.. pls teach me Praniüòú\", '1']]\n"
     ]
    }
   ],
   "source": [
    "#import gensim\n",
    "import json\n",
    "import csv  \n",
    "\n",
    "with open(r'E:\\Semesters\\Spring_2022_break\\Machine Learning\\TEST1.CSV','r',encoding ='Utf-8')as csv_read:\n",
    "    csv_reader =csv.reader(csv_read)\n",
    "    list1 = list(csv_reader)\n",
    "\n",
    "\n",
    "    print(list1[:10])\n",
    "\n",
    "#print(csv_read)\n",
    "#gendata = []\n",
    "#for lists in list1[:20]:\n",
    "   # for txt in lists:\n",
    "  #      #print(txt)\n",
    " #       gendata.append(gensim.utils.simple_preprocess(txt))\n",
    "\n",
    "#print(gendata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to_lower_case_data =[]\n",
    "def to_lower_case(data):\n",
    "    to_lower_case_data =[]\n",
    "    #global to_lower_case_data\n",
    "    for lists in data:\n",
    "        data_sublist =[]\n",
    "        for words in lists:\n",
    "            data_sublist.append(words.lower())\n",
    "        to_lower_case_data.append(data_sublist)\n",
    "    return to_lower_case_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x=to_lower_case(list1[:5471])\n",
    "#print(to_lower_case_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#characters_removed =[]\n",
    "def remove_characters(to_lower_case_data):\n",
    "    characters_removed =[]\n",
    "    for lists in to_lower_case_data:\n",
    "        clean =[]\n",
    "        #print(lists)\n",
    "        for scentences in lists:\n",
    "            #print(scentences)\n",
    "            for words in scentences:\n",
    "                if is_emoji(words) is False:\n",
    "                    regular = re.sub(r'[^\\w\\s]',\"\",scentences)\n",
    "                elif is_emoji(words) is True:\n",
    "                    emj= words\n",
    "                    clean.append(emj)\n",
    "            clean.append(regular)\n",
    "            \n",
    "                #print(clean)\n",
    "        characters_removed.append(clean)\n",
    "    return characters_removed\n",
    "\n",
    "\n",
    "#remove_characters(to_lower_case_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords_removed =[]\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(to_lower_case_data):\n",
    "    stopwords_removed =[]\n",
    "    for lists in to_lower_case_data:\n",
    "        clean=[]\n",
    "        for words in lists:\n",
    "            if not words in stopwords.words('english'):\n",
    "                clean.append(words)\n",
    "        stopwords_removed.append(clean)\n",
    "    return stopwords_removed\n",
    "\n",
    "\n",
    "#x=remove_stopwords(to_lower_case_data)\n",
    "#print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emoji import UNICODE_EMOJI\n",
    "\n",
    "# search your emoji\n",
    "def is_emoji(s, language=\"en\"):\n",
    "    return s in UNICODE_EMOJI[language]\n",
    "\n",
    "# add space near your emoji\n",
    "def add_space_between_emojis(text):\n",
    "    return ''.join(' ' + char if is_emoji(char) else char for char in text).strip()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#tokenized_words_1 = []\n",
    "def tokenize_word(data):\n",
    "    tokenized_words_1 = []\n",
    "    for lists in data:\n",
    "        data_sublist =[]\n",
    "        for sentences in lists:\n",
    "            data_sublist.append(word_tokenize(add_space_between_emojis(sentences)))\n",
    "        tokenized_words_1.append(data_sublist)\n",
    "    return tokenized_words_1\n",
    "\n",
    "#x = tokenize_word(stopwords_removed)\n",
    "\n",
    "#print(x)\n",
    "#tokenize_word(characters_removed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentimental_value(token_words):\n",
    "    words_sentimental_value = 0\n",
    "    emoji_sentimental_value = 0\n",
    "    for lists in token_words:\n",
    "        for words in lists:\n",
    "            for character in words:\n",
    "                if ord(character) > 127:        #for emoji sentimental value \n",
    "                    try:\n",
    "                        emoji_sentimental_value += emoji_parsed[words]\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                elif ord(character) < 127 and character == words[-1]: # word sentimental val (only increment the sentimental val on last char of the word)\n",
    "                    try:\n",
    "                        words_sentimental_value += lables_parsed[words]\n",
    "                    except:\n",
    "                        pass\n",
    "    return [words_sentimental_value, emoji_sentimental_value]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#sentm_data_3d_list =[]\n",
    "#sentm_data_seperated_list= []\n",
    "def get_sentm_data_in_3d_and_seprate(tokenized_words_1):\n",
    "    sentm_data_3d_list =[]\n",
    "    sentm_data_seperated_list= []\n",
    "    for lists in tokenized_words_1:\n",
    "        #print(lists[1:])\n",
    "        for sublist in lists[1:]:\n",
    "            #print(sublist)\n",
    "            for sarcastic_tag in sublist:\n",
    "                pass\n",
    "        sarcastic_val = sentimental_value(lists)\n",
    "        sentm_data_3d_list.append([sarcastic_val,sublist])\n",
    "        sentm_data_seperated_list.append([sarcastic_val,sarcastic_tag])\n",
    "    return sentm_data_3d_list, sentm_data_seperated_list\n",
    "    \n",
    "    \n",
    "\n",
    "#x,y = get_sentm_data_in_3d_and_seprate(tokenized_words_1)\n",
    "#print(x[:5])\n",
    "#print(y[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentm_data_2d_list =[]\n",
    "def get_data_in_2d_list(tokenized_words_1):\n",
    "    sentm_data_2d_list =[]\n",
    "    for lists in tokenized_words_1:\n",
    "        #print(lists[1:])\n",
    "        for sublist in lists[1:]:\n",
    "            #print(sublist)\n",
    "            for words in sublist:\n",
    "                pass\n",
    "        x = sentimental_value(lists)\n",
    "        x.append(words)\n",
    "        sentm_data_2d_list.append(x)\n",
    "    return sentm_data_2d_list\n",
    "\n",
    "#get_data_in_2d_list(tokenized_words_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lables = []\n",
    "#features = []\n",
    "def get_feature_lables_list(sentm_data):\n",
    "    lables = []\n",
    "    features = []\n",
    "    for lists in sentm_data:\n",
    "        #print(lists[:1])\n",
    "        for sublist_for_features in lists[:1]:\n",
    "            #print(sublist_for_features)\n",
    "            features.append(sublist_for_features)\n",
    "        for sublist_for_lables in lists[1:]:\n",
    "                #print(sublist_for_lables)\n",
    "                lables.append(sublist_for_lables)\n",
    "    return features , lables\n",
    "\n",
    "\n",
    "\n",
    "#x,y=get_feature_lables_list(sentm_data_seperated_list)\n",
    "#print(x[:5])\n",
    "#print(y[:5])\n",
    "\n",
    "#features[1:]\n",
    "#lables[1:]\n",
    "\n",
    "#lables[1:][0]\n",
    "# print(features[1], lables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def train_the_model(TwoDimension_list):\n",
    "    lower = to_lower_case(list1)\n",
    "    rem = remove_stopwords(lower)\n",
    "    tk_word = tokenize_word(rem)\n",
    "    sentm1,sentm2 = get_sentm_data_in_3d_and_seprate(tk_word)\n",
    "    features,lables = get_feature_lables_list(sentm2)\n",
    "\n",
    "    clf = KNeighborsClassifier()\n",
    "    clf.fit(features[1:],lables[1:])\n",
    "\n",
    "#preds = clf.predict([[3,2]])\n",
    "    print(\"Training Sucessfull!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning Sucessfull!\n"
     ]
    }
   ],
   "source": [
    "train_the_model(list1[:5471])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_for_sarcasm(TwoDimension_list):\n",
    "    lower = to_lower_case(TwoDimension_list)\n",
    "    rem = remove_stopwords(lower)\n",
    "    tk_word = tokenize_word(rem)\n",
    "    sentm1,sentm2 = get_sentm_data_in_3d_and_seprate(tk_word)\n",
    "    feature,lables = get_feature_lables_list(sentm2)\n",
    "    result = clf.predict(feature)\n",
    "    if result == '1':\n",
    "        return print(user_input,\"--> IS Sarcastic\")\n",
    "    elif result == '0':\n",
    "        return print(user_input1,\"--> IS NON Sarcastic\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Wasi ! You Have Done a great joob üòÇüòÇ', '0']] --> IS Sarcastic\n",
      "[['Wasi ! You Have Done a great joob üòçüòç', '0']] --> IS NON Sarcastic\n"
     ]
    }
   ],
   "source": [
    "#list1[:5471] FOR TRAINING\n",
    "user_input =[[\"Wasi ! You Have Done a great joob üòÇüòÇ\",\"0\"]]    #FOR Testing\n",
    "user_input1 =[[\"Wasi ! You Have Done a great joob üòçüòç\",\"0\"]]\n",
    "\n",
    "test_for_sarcasm(user_input)\n",
    "test_for_sarcasm(user_input1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "30424d9308010dafecca22b5ec9402eef0e44005ea326a16069b35614aa41d25"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
